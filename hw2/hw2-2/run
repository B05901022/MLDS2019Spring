# -*- coding: utf-8 -*-
"""
Created on Wed Apr 17 14:22:07 2019
@author: Austin Hsu
"""
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = "3"

import numpy as np
from gensim.models import word2vec
from tqdm import tqdm
from sklearn.preprocessing import normalize
from transformer_tutorial import make_model, subsequent_mask, Generator

import torch
import torchvision
import torch.nn as nn
import torch.utils.data as Data
import torch.functional as F



###DEVICE###
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

###REPRODUCIBLE###
torch.manual_seed(1)

###HYPERPARAMETER###
EPOCH      = 2
BATCHSIZE  = 1
ADAMPARAM  = {'lr':0.001, 'betas':(0.9, 0.999), 'eps':1e-08}#, 'weight_decay':1e-05}

###DATA LOADING PARAMS###
LOADPARAM  = {'directory': './MLDS_hw2_1_data', 'min_count':3, 'random_seed':None, 'batch_size':4}

def load_dataset(word2idx,
                 directory='./../mlds_hw2_2_data/clr_conversation.txt',
                 pad_len=20,
                 min_len=2,
                 ):
    
    dataset = open(directory, 'r', encoding='UTF-8').read().split('+++$+++')
    dataset = [[j.split(' ') for j in i.split('\n') if j != ''] for i in dataset]
    
    print("Generating dataset...")
    train_x = []
    train_y = []
    register= []
    for valid in dataset:
        for sent in valid:
            sent_len = len(sent)
            if sent_len <= (pad_len - 2) and sent_len >= min_len:
                padded_sent = [1]
                padded_sent += sent2idx(sent, word2idx)
                padded_sent += [2]
                padded_sent += [0] * (pad_len - sent_len - 2)
                if register != []:
                    train_x.append(register)
                    train_y.append(padded_sent)
                    register = padded_sent
                else:
                    register = padded_sent
            else:
                register = []
        register = []
    print("Generating finished.")  
    
    train_x = np.array(train_x)
    train_y = np.array(train_y)
    
    return train_x, train_y

def sent2idx(sentence,
             word2idx,
             ):
    idxsent = []
    for word in sentence:
        try:
            idxsent.append(word2idx[word])
        except:
            idxsent.append(3)
    return idxsent

def word2vec_model(directory='./../mlds_hw2_2_data/clr_conversation.txt',
                   model_name='word2vec_only_train.model',
                   pre=False
                   ):
    
    """
    Parse the TRAINING data and generate an embedding relationship.
    input: directory of dataset, 
           model name of word to vector model,
           prebuild model or not(False: generate a new model)
    output: parsed dataset, word to vector model
    
    TODO: Make a similar function for test data parsing
    """
    
    dataset = open(directory, 'r', encoding='UTF-8').read().split('+++$+++')
    dataset = [[j.split(' ') for j in i.split('\n') if j != ''] for i in dataset]
    
    if not pre:
        sentences = []
        for sent in dataset:
            sentences += sent
        
        print('Generating word2vec model')
        word2vec_model = word2vec.Word2Vec(sentences=sentences, size=250, window=5, min_count=5, workers=16, iter=100, sg=1)
        word2vec_model.save(model_name)
        print('Finished word2vec model')
    else:
        word2vec_model = word2vec.Word2Vec.load(model_name)
        
    dataset = np.array(dataset)
    
    return dataset, word2vec_model

def text_to_index(corpus,
                  word2idx,
                  min_sent_len=2,
                  max_sent_len=18,
                  max_unk=2,
                  pad_len=20
                  ):
    
    """
    Converts text to index.
    input: parsed corpus, word to index matrix
    output: (available_dialogue, sentences, word index)
    """
    
    new_corpus = []
    for doc in corpus:
        new_doc = []
        for sent in doc:
            new_sent = []
            sent_len = 0
            unk = 0
            for word in sent:
                sent_len += 1
                try:
                    new_sent.append(word2idx[word])
                except:
                    new_sent.append(3)# idx2word[3]='UNK'
                    unk += 1
            if unk <= max_unk and sent_len >= min_sent_len and sent_len <= max_sent_len:
                new_doc.append(np.array([1]+new_sent +[2]+[0] * (pad_len - len(new_sent)-2)))#Add EOS and BOS signal
            else:
                new_doc.append(np.zeros(20,))
        new_corpus.append(np.array(new_doc))
    return np.array(new_corpus)

def valid_dialogue(idx_corpus,
                   ):
    
    """
    Generate valid training data.
    input: indexed corpus
    output: train_x, train_y
    """
    
    train_x = []
    train_y = []
    cut_tag = np.zeros(20,)
    for available_dialogue in idx_corpus:
        for sent in range(len(available_dialogue)-1):
            #print(available_dialogue[sent] == cut_tag)
            if (available_dialogue[sent] == cut_tag).all() or (available_dialogue[sent+1] == cut_tag).all():
                pass
            else:
                train_x.append(available_dialogue[sent])
                train_y.append(available_dialogue[sent+1])
    return np.array(train_x), np.array(train_y)

def embedding_idx(corpus,
                  embedding_matrix
                  ):
    
    """
    Converts indexes to vector of size 250 for model input.
    input: corpus(train_x, train_y or so), embedding matrix
    output: vectors of size 250
    """
    
    new_corpus = []
    for sent in corpus:
        new_sent = []
        for word in sent:
            try:
                new_sent.append(embedding_matrix[int(word)])
            except:
                new_sent.append(np.zeros(20,))
        new_corpus.append(new_sent)
    return np.array(new_corpus)

def recover(corpus,
            embedding_matrix_normalized,
            idx2word
            ):
    
    """
    Recovers vectors of dim=250 to words by cosine similarity.
    input: model output, NORMALIZED embedding matrix, index to word dictionary
    output: sentences(list of words)
    """
    
    new_corpus = []
    for sent in corpus:
        new_sent = []      
        for word in sent:
            if np.linalg.norm(word) != 0:
                new_word = np.argmax(np.dot(embedding_matrix_normalized,
                                            word / np.linalg.norm(word)), axis=0)
                try:
                    new_sent.append(idx2word[new_word])
                except:
                    pass
        new_corpus.append(new_sent)      
    return new_corpus

def main():
    
    _ , w2v_model = word2vec_model(pre=True)#pre=True
    
    ###EMBEDDING###
    embedding_matrix = np.zeros((len(w2v_model.wv.vocab.items()) + 4, w2v_model.vector_size))
    word2idx = {}

    vocab_list = [(word, w2v_model.wv[word]) for word, _ in w2v_model.wv.vocab.items()]
    for i, vocab in enumerate(vocab_list):
        word, vec = vocab
        embedding_matrix[i + 4] = vec
        word2idx[word] = i + 4
    word2idx['<PAD>'] = 0
    word2idx['<BOS>'] = 1
    word2idx['<EOS>'] = 2
    word2idx['<UNK>'] = 3
    idx2word = {word2idx[i]:i for i in word2idx.keys()}
    
    embedding_matrix_normalized = normalize(embedding_matrix, axis=1)
    
    """
    dataset = text_to_index(dataset, word2idx)#dataset:(available dialogue, sentences, word_index)
    
    train_x, train_y = valid_dialogue(dataset)
    """
    """
    #will cause OOM 
    train_x = embedding_idx(train_x, embedding_matrix=embedding_matrix)
    train_y = embedding_idx(train_y, embedding_matrix=embedding_matrix)
    """
    
    Transformer_model = make_model(src_vocab = 71475,
                                   tgt_vocab = 71475,
                                   ).cuda()
    
    train_x, train_y = load_dataset(word2idx=word2idx)
    print(train_x.shape, ',', train_y.shape)
    #train_x = embedding_idx(train_x, embedding_matrix=embedding_matrix)
    #train_y = embedding_idx(train_y, embedding_matrix=embedding_matrix)
    tensor_x = torch.stack([torch.from_numpy(np.array(i)) for i in train_x])
    tensor_y = torch.stack([torch.from_numpy(np.array(i)) for i in train_y])
    
    print(tensor_x.shape, ',', tensor_y.shape)
    
    train_dataset = Data.TensorDataset(tensor_x,tensor_y) # create your datset
    train_dataloader = Data.DataLoader(train_dataset)#batch_size=BATCHSIZE) # create your dataloader
    
    print('dataloader complete')
    
    ###OPTIMIZER###
    optimizer = torch.optim.Adam(Transformer_model.parameters(), **ADAMPARAM)
    #optimizer = torch.optim.Adam(Transformer_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))
    
    ###LOSS FUNCTION###
    loss_func = nn.CrossEntropyLoss()
    
    print("Training starts...")
    
    history_best_epoch_loss = 1.0
    loss_list = []
    
    for e in range(EPOCH):
        print("Epoch ", e+1)
        epoch_loss = 0
        
        for b_num, (b_x, b_y) in enumerate(tqdm(train_dataloader)):
            mask = subsequent_mask(20).cuda()

            b_x = b_x.cuda()
            b_y = b_y.cuda()
            optimizer.zero_grad()
            pred = Transformer_model(b_x, b_y, mask, mask)

            pred = Transformer_model.generator(pred)
            loss = loss_func(pred.contiguous().view(-1, pred.size(-1)),  b_y.contiguous().view(-1))
            loss.backward(retain_graph=True)
            optimizer.step()
            epoch_loss += loss.item()
            #print('Step ', b_num, ', loss :', loss.item())
            if(b_num % 10000 == 9999 ):
                torch.save(Transformer_model, './models/'+'epoch_'+str(b_num)+'_Transformer_model.pkl')
                torch.save(optimizer.state_dict(), './models/'+'epoch'+str(b_num)+'_model.optim')
            
        current_epoch_loss = epoch_loss / datasize
        loss_list.append(current_epoch_loss)
        
        history_best_epoch_loss = min(current_epoch_loss,history_best_epoch_loss)
        print("Epoch loss: ", current_epoch_loss)
        
    np.save('./loss_record/'+'model_loss', np.array(loss_list))
    print("Best loss : %.8f" % history_best_epoch_loss)
    print("Training finished.")
    
    
    #return embedding_matrix, embedding_matrix_normalized, train_x, train_y, word2idx, idx2word

main()
print('done main')
# -*- coding: utf-8 -*-
"""
Created on Tue Apr 30 20:33:16 2019
@author: Austin Hsu
"""

"""
Reference:
    https://zhuanlan.zhihu.com/p/48731949
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
#import seaborn

#seaborn.set_context(context="talk")

"""
Encoder-Decoder Structure
Generator of results
"""

class EncoderDecoder(nn.Module):
    def __init__(self,
                 encoder,
                 decoder,
                 src_embed,
                 tgt_embed,
                 generator
                 ):
        super(EncoderDecoder, self).__init__()
        self.encoder   = encoder
        self.decoder   = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
    def forward(self,
                src,
                tgt,
                src_mask,
                tgt_mask,
                ):
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)

class Generator(nn.Module):
    def __init__(self,
                 d_model,
                 vocab
                 ):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)
    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1)

"""
Encoder
"""
def clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])
    
class Encoder(nn.Module):
    def __init__(self,
                 layer,
                 N=6
                 ):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm   = LayerNorm(layer.size)
    def forward(self, x, mask):
        for single_layer in self.layers:
            x = single_layer(x, mask)
        return self.norm(x)

class LayerNorm(nn.Module):
    def __init__(self,
                 features,
                 eps=1e-6
                 ):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps
    def forward(self, x):
        mean   = x.mean(-1, keepdim=True)
        std    = x.std(-1, keepdim=True)
        output = self.a_2 * ( x - mean ) / ( std + self.eps) + self.b_2
        return output
    
class SublayerConnection(nn.Module):
    def __init__(self,
                 size,
                 dropout
                 ):
        super(SublayerConnection, self).__init__()
        self.norm    = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x, sublayer):
        sublayer_output = self.dropout(self.norm(x))
        output = x + sublayer_output
        return output

class EncoderLayer(nn.Module):
    def __init__(self,
                 size,
                 self_attn,
                 feed_forward,
                 dropout
                 ):
        super(EncoderLayer, self).__init__()
        self.self_attn    = self_attn
        self.feed_forward = feed_forward
        self.sublayer     = clones(SublayerConnection(size, dropout), 2)
        self.size         = size
    def forward(self, x, mask):
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        output = self.sublayer[1](x, self.feed_forward)
        return output

"""
Decoder
"""

class Decoder(nn.Module):
    def __init__(self,
                 layer,
                 N=6
                 ):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm   = LayerNorm(layer.size)
    def forward(self, x, memory, src_mask, tgt_mask):
        for single_layer in self.layers:
            x = single_layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)

class DecoderLayer(nn.Module):
    def __init__(self,
                 size,
                 self_attn,
                 src_attn,
                 feed_forward,
                 dropout
                 ):
        super(DecoderLayer, self).__init__()
        self.size         = size
        self.self_attn    = self_attn
        self.src_attn     = src_attn
        self.feed_forward = feed_forward
        self.sublayer     = clones(SublayerConnection(size, dropout), 3)
    def forward(self, x, memory, src_mask, tgt_mask):
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        output = self.sublayer[2](x, self.feed_forward)
        return output

"""
Mask
"""

def subsequent_mask(size):
    """
    Make sure later informations won't affect prediction of present timestep
    """
    attn_shape = (1, size, size)
    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')
    
    """
    np.triu
        means the upper triangle part of a matrix,
        k=1 means only parts that are above the diagonal part are leaved
        i.e. 
        a = array([[ 1, 2, 3],
                   [ 4, 5, 6],
                   [ 7, 8, 9],
                   [10,11,12]])
        np.triu(a, k=1)
         == array([[ 0, 2, 3],
                   [ 0, 0, 6],
                   [ 0, 0, 0],
                   [ 0, 0, 0]])
    """
    return torch.from_numpy(mask) == 0

"""
Attention
"""

def attention(query, key, value, mask=None, dropout=None):
    """
    
       Q       K       V
       |       |       |
      [ Matmul  ]      |
       |       |       |
      [ Scale   ]      |
       |       |       |
      [  Mask   ]      |
       |       |       |
      [ Softmax ]      |
       |       |       |
      [       Matmul     ]
                 |
              Output
      
    """
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.mask_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

"""
Multi-Headed Attention
"""

class MultiHeadedAttention(nn.Module):
    def __init__(self,
                 h,
                 d_model,
                 dropout=0.1
                 ):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        self.d_k = d_model // h
        self.h   = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)
    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
            for l, x in zip(self.linears, (query, key, value))]
        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)
        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)
        
"""
Positionwise Feed Forward
"""

class PositionwiseFeedForward(nn.Module):
    def __init__(self,
                 d_model,
                 d_ff,
                 dropout=0.1
                 ):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1     = nn.Linear(d_model, d_ff)
        self.w_2     = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(p=dropout)
    def forward(self, x):
        x      = self.w_1(x)
        x      = F.relu(x)
        x      = self.dropout(x)
        output = self.w_2(x)
        return output
    
"""
Embedding
"""

class Embeddings(nn.Module):
    def __init__(self,
                 d_model,
                 vocab
                 ):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model
    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)
    
"""
Positional Encoding
"""

class PositionalEncoding(nn.Module):
    def __init__(self,
                 d_model,
                 dropout,
                 max_len=5000
                 ):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        pe           = torch.zeros(max_len, d_model)
        position     = torch.arange(0. , max_len).unsqueeze(1)
        div_term     = torch.exp(torch.arange(0. , d_model, 2) * -(math.log(10000.0) / d_model))
        pe[:, 0::2]  = torch.sin(position * div_term)
        pe[:, 1::2]  = torch.cos(position * div_term)
        pe           = pe.unsqueeze(0)
        #pe           = pe[:, :20]
        #pe           = pe.squeeze(0)
        #pe           = pe.transpose(1,0)
        #pe           = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        #print('x : ',x.shape)
        #print('pe : ',Variable(self.pe[:, :x.size(1)], requires_grad=False).shape)
        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)
        return self.dropout(x)

"""
Full Model
"""

def make_model(src_vocab,
               tgt_vocab,
               N=6,
               d_model=512,
               d_ff=2048,
               h=8,
               dropout=0.1
               ):
    c = copy.deepcopy
    attn = MultiHeadedAttention(h, d_model)
    ff   = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
                Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
                Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
                nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
                nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
                Generator(d_model, tgt_vocab)
                )
    
    for p in model.parameters():
        if p.dim() > 1:
            nn.init.xavier_uniform(p)
    
    return model
'''
tmp_model = make_model(250,250,6)
print('model complete')
None
'''
'''
class SimpleLossCompute:
    "A simple loss compute and train function."
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt
        
    def __call__(self, x, y, norm):
        x = self.generator(x)
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), 
                              y.contiguous().view(-1)) / norm
        loss.backward()
        if self.opt is not None:
            self.opt.step()
            self.opt.optimizer.zero_grad()
        return loss.data[0] * norm
'''       
